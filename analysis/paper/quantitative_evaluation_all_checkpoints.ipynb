{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "# Define a scaling factor\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes by multiplying with the scaling factor\n",
    "rcParams['font.size'] *= scale_factor       # Default font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seed = 1\n",
    "test_seed = 101\n",
    "env_id = \"CARLPendulum\"\n",
    "test_env_config_ids = [f\"test_{i}\" for i in range(1, 25+1)]\n",
    "c1_values = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "c2_values = [0.1, 0.5, 1.0, 1.5, 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_er = defaultdict(list)\n",
    "mean_er_std = defaultdict(list)\n",
    "min_er = defaultdict(list)\n",
    "worst_k_mean_er = defaultdict(list)\n",
    "worst_k_mean_er_std = defaultdict(list)\n",
    "\n",
    "for checkpoint_idx in range(5000, 50001, 5000):\n",
    "    print(checkpoint_idx)\n",
    "\n",
    "    return_means = defaultdict(list)\n",
    "    returns = defaultdict(list)\n",
    "\n",
    "    env_config_id = \"train\"\n",
    "\n",
    "    for method in [\"stacked\", \"stacked_bdr\"]:\n",
    "        for test_env_config_id in test_env_config_ids:\n",
    "            checkpoint_directory = f\"../../runs/test/seed_{training_seed}/{env_id}/{test_env_config_id}/{env_config_id}/{method}/checkpoint_{checkpoint_idx}\"\n",
    "            if not os.path.exists(checkpoint_directory):\n",
    "                continue\n",
    "            filenames = os.listdir(checkpoint_directory)\n",
    "            for filename in filenames:\n",
    "                if filename.startswith(\"event\"):\n",
    "                    event_filename = filename\n",
    "                    break\n",
    "            event_filename = os.path.join(checkpoint_directory, event_filename)\n",
    "\n",
    "            event_acc = EventAccumulator(event_filename)\n",
    "            event_acc.Reload()\n",
    "\n",
    "            for e in event_acc.Scalars(f\"charts/checkpoint_{checkpoint_idx}/episodic_return\"):\n",
    "                # checkpoint_idxs[method].append(e.step)\n",
    "                returns[method].append(e.value)\n",
    "\n",
    "            for e in event_acc.Scalars(f\"evaluation/seed_{test_seed}/episodic_return_mean\"):\n",
    "                # checkpoint_idxs[method].append(e.step)\n",
    "                return_means[method].append(e.value)\n",
    "        \n",
    "    for method in returns:\n",
    "        if method == 'optimal':\n",
    "            continue\n",
    "        \n",
    "        print(method)\n",
    "        k = 0.1\n",
    "        worst_k = int(len(returns[method])*k)\n",
    "        worst_k_returns = sorted(returns[method])[:worst_k]\n",
    "\n",
    "        print(f'Mean Episodic Reward: {np.mean(return_means[method])} +- {np.std(return_means[method])}')\n",
    "        print(f'Minimum Episodic Reward: {np.min(return_means[method])}')\n",
    "        print(f'Mean Episodic Reward (Worst {int(k*100)}%): {np.mean(worst_k_returns)} +- {np.std(worst_k_returns)}')\n",
    "        print('='*40)\n",
    "\n",
    "        mean_er[method].append(np.mean(return_means[method]))\n",
    "        mean_er_std[method].append(np.std(return_means[method]))\n",
    "        min_er[method].append(np.min(return_means[method]))\n",
    "        worst_k_mean_er[method].append(np.mean(worst_k_returns))\n",
    "        worst_k_mean_er_std[method].append(np.std(worst_k_returns))\n",
    "    print('='*40)\n",
    "\n",
    "for method in mean_er:\n",
    "    mean_er[method] = np.array(mean_er[method])\n",
    "    mean_er_std[method] = np.array(mean_er_std[method])\n",
    "    min_er[method] = np.array(min_er[method])\n",
    "    worst_k_mean_er[method] = np.array(worst_k_mean_er[method])\n",
    "    worst_k_mean_er_std[method] = np.array(worst_k_mean_er_std[method])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "methods = ['Baseline', 'BDR']\n",
    "markers = ['o', 's']\n",
    "metrics = ['Minimum Episodic Return', 'Mean Episodic Return (Worst 10%)', 'Mean Episodic Return']\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 6))\n",
    "\n",
    "for j, method in enumerate(['stacked', 'stacked_bdr']):\n",
    "    x = np.arange(1, len(mean_er[method])+1)\n",
    "\n",
    "    mean_er[method][:2] = np.nan\n",
    "    mean_er_std[method][:2] = np.nan\n",
    "    min_er[method][:2] = np.nan\n",
    "    worst_k_mean_er[method][:2] = np.nan\n",
    "    worst_k_mean_er_std[method][:2] = np.nan\n",
    "\n",
    "    # Minimum Episodic Return\n",
    "    axs[0].plot(x, min_er[method], marker=markers[j], label=methods[j])\n",
    "    \n",
    "    # Mean Episodic Return (Worst 10%)\n",
    "    axs[1].plot(x, worst_k_mean_er[method], marker=markers[j], label=methods[j])\n",
    "    # axs[1].fill_between(np.arange(len(mean_er[method])), worst_k_mean_er[method] - worst_k_mean_er_std[method], worst_k_mean_er[method] + worst_k_mean_er_std[method], alpha=0.2)\n",
    "\n",
    "    # Mean Episodic Return\n",
    "    axs[2].plot(x, mean_er[method], marker=markers[j], label=methods[j])\n",
    "    # axs[2].fill_between(np.arange(len(mean_er[method])), mean_er[method] - mean_er_std[method], mean_er[method] + mean_er_std[method], alpha=0.2)\n",
    "    \n",
    "    for i in range(3):\n",
    "        axs[i].legend(loc=4)\n",
    "        axs[i].set_xlabel('Training Time', fontsize=18)\n",
    "        axs[i].set_ylabel('Episodic Return', fontsize=18)\n",
    "        axs[i].set_title(metrics[i])\n",
    "        axs[i].set_xlim(1-0.2, len(x)+0.2)\n",
    "        if i == 2:\n",
    "            axs[i].set_ylim(-850, -350)\n",
    "        else:\n",
    "            axs[i].set_ylim(-1550, -1050)\n",
    "        axs[i].set_xticks(x)\n",
    "        axs[i].axvline(x=3, linestyle='--', color='gray')\n",
    "\n",
    "fig.suptitle('Pendulum - Checkpoint Evaluation Performance Comparison')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "plt.savefig('output/CARLPendulum/checkpoint_evaluation.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seed = 1\n",
    "test_seed = 101\n",
    "env_id = \"CARLDmcWalkerEnv\"\n",
    "test_env_config_ids = [f\"test_{i}\" for i in range(1, 25+1)]\n",
    "c1_values = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "c2_values = [1, 5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_er = defaultdict(list)\n",
    "mean_er_std = defaultdict(list)\n",
    "min_er = defaultdict(list)\n",
    "worst_k_mean_er = defaultdict(list)\n",
    "worst_k_mean_er_std = defaultdict(list)\n",
    "\n",
    "for checkpoint_idx in range(50000, 500001, 50000):\n",
    "    print(checkpoint_idx)\n",
    "\n",
    "    return_means = defaultdict(list)\n",
    "    returns = defaultdict(list)\n",
    "\n",
    "    env_config_id = \"train\"\n",
    "\n",
    "    for method in [\"stacked\", \"stacked_bdr\"]:\n",
    "        for test_env_config_id in test_env_config_ids:\n",
    "            checkpoint_directory = f\"../../runs/test/seed_{training_seed}/{env_id}/{test_env_config_id}/{env_config_id}/{method}/checkpoint_{checkpoint_idx}\"\n",
    "            if not os.path.exists(checkpoint_directory):\n",
    "                continue\n",
    "            filenames = os.listdir(checkpoint_directory)\n",
    "            for filename in filenames:\n",
    "                if filename.startswith(\"event\"):\n",
    "                    event_filename = filename\n",
    "                    break\n",
    "            event_filename = os.path.join(checkpoint_directory, event_filename)\n",
    "\n",
    "            event_acc = EventAccumulator(event_filename)\n",
    "            event_acc.Reload()\n",
    "\n",
    "            for e in event_acc.Scalars(f\"charts/checkpoint_{checkpoint_idx}/episodic_return\"):\n",
    "                # checkpoint_idxs[method].append(e.step)\n",
    "                returns[method].append(e.value)\n",
    "\n",
    "            for e in event_acc.Scalars(f\"evaluation/seed_{test_seed}/episodic_return_mean\"):\n",
    "                # checkpoint_idxs[method].append(e.step)\n",
    "                return_means[method].append(e.value)\n",
    "        \n",
    "    for method in returns:\n",
    "        if method == 'optimal':\n",
    "            continue\n",
    "        \n",
    "        print(method)\n",
    "        k = 0.1\n",
    "        worst_k = int(len(returns[method])*k)\n",
    "        worst_k_returns = sorted(returns[method])[:worst_k]\n",
    "\n",
    "        print(f'Mean Episodic Reward: {np.mean(return_means[method])} +- {np.std(return_means[method])}')\n",
    "        print(f'Minimum Episodic Reward: {np.min(return_means[method])}')\n",
    "        print(f'Mean Episodic Reward (Worst {int(k*100)}%): {np.mean(worst_k_returns)} +- {np.std(worst_k_returns)}')\n",
    "        print('='*40)\n",
    "\n",
    "        mean_er[method].append(np.mean(return_means[method]))\n",
    "        mean_er_std[method].append(np.std(return_means[method]))\n",
    "        min_er[method].append(np.min(return_means[method]))\n",
    "        worst_k_mean_er[method].append(np.mean(worst_k_returns))\n",
    "        worst_k_mean_er_std[method].append(np.std(worst_k_returns))\n",
    "    print('='*40)\n",
    "\n",
    "for method in mean_er:\n",
    "    mean_er[method] = np.array(mean_er[method])\n",
    "    mean_er_std[method] = np.array(mean_er_std[method])\n",
    "    min_er[method] = np.array(min_er[method])\n",
    "    worst_k_mean_er[method] = np.array(worst_k_mean_er[method])\n",
    "    worst_k_mean_er_std[method] = np.array(worst_k_mean_er_std[method])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "methods = ['Baseline', 'BDR']\n",
    "markers = ['o', 's']\n",
    "metrics = ['Minimum Episodic Return', 'Mean Episodic Return (Worst 10%)', 'Mean Episodic Return']\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 6))\n",
    "\n",
    "for j, method in enumerate(['stacked', 'stacked_bdr']):\n",
    "    x = np.arange(1, len(mean_er[method])+1)\n",
    "\n",
    "    # Minimum Episodic Return\n",
    "    axs[0].plot(x, min_er[method], marker=markers[j], label=methods[j])\n",
    "    \n",
    "    # Mean Episodic Return (Worst 10%)\n",
    "    axs[1].plot(x, worst_k_mean_er[method], marker=markers[j], label=methods[j])\n",
    "    # axs[1].fill_between(np.arange(len(mean_er[method])), worst_k_mean_er[method] - worst_k_mean_er_std[method], worst_k_mean_er[method] + worst_k_mean_er_std[method], alpha=0.2)\n",
    "\n",
    "    # Mean Episodic Return\n",
    "    axs[2].plot(x, mean_er[method], marker=markers[j], label=methods[j])\n",
    "    # axs[2].fill_between(np.arange(len(mean_er[method])), mean_er[method] - mean_er_std[method], mean_er[method] + mean_er_std[method], alpha=0.2)\n",
    "    \n",
    "    for i in range(3):\n",
    "        axs[i].legend(loc=4)\n",
    "        axs[i].set_xlabel('Training Time', fontsize=18)\n",
    "        axs[i].set_ylabel('Episodic Return', fontsize=18)\n",
    "        axs[i].set_title(metrics[i])\n",
    "        axs[i].set_xlim(1-0.2, len(x)+0.2)\n",
    "        if i == 2:\n",
    "            axs[i].set_ylim(1, 950)\n",
    "        else:\n",
    "            axs[i].set_ylim(1, 950)\n",
    "        axs[i].set_xticks(x)\n",
    "        axs[i].axvline(x=3, linestyle='--', color='gray')\n",
    "\n",
    "fig.suptitle('Walker - Checkpoint Evaluation Performance Comparison')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "plt.savefig('output/CARLDmcWalkerEnv/checkpoint_evaluation.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
