{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "# Define a scaling factor\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes by multiplying with the scaling factor\n",
    "rcParams['font.size'] *= scale_factor       # Default font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seed = 1\n",
    "test_seed = 101\n",
    "env_id = \"CARLPendulum\"\n",
    "test_env_config_ids = [f\"test_{i}\" for i in range(1, 25+1)]\n",
    "c1_values = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "c2_values = [0.1, 0.5, 1.0, 1.5, 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config_id = \"default\"\n",
    "\n",
    "# checkpoint_idxs = defaultdict(list)\n",
    "return_means = defaultdict(list)\n",
    "return_stds = defaultdict(list)\n",
    "returns = defaultdict(list)\n",
    "\n",
    "\n",
    "# method = 'optimal'\n",
    "# for test_env_config_id in test_env_config_ids:\n",
    "#     checkpoint_directory = f\"../../runs/test/seed_{training_seed}/{env_id}/{test_env_config_id}/{test_env_config_id}/stacked\"\n",
    "#     if not os.path.exists(checkpoint_directory):\n",
    "#         return_means[method].append(1)\n",
    "#         return_stds[method].append(1)\n",
    "#         continue\n",
    "#     filenames = os.listdir(checkpoint_directory)\n",
    "#     for filename in filenames:\n",
    "#         if filename.startswith(\"event\"):\n",
    "#             event_filename = filename\n",
    "#             break\n",
    "#     event_filename = os.path.join(checkpoint_directory, event_filename)\n",
    "\n",
    "#     event_acc = EventAccumulator(event_filename)\n",
    "#     event_acc.Reload()\n",
    "\n",
    "#     # print(event_acc.Tags())\n",
    "\n",
    "#     try:\n",
    "#         for e in event_acc.Scalars(f\"charts/episodic_return\"):\n",
    "#             # checkpoint_idxs[method].append(e.step)\n",
    "#             returns[method].append(e.value)\n",
    "            \n",
    "#         for e in event_acc.Scalars(f\"evaluation/seed_{test_seed}/episodic_return_mean\"):\n",
    "#             # checkpoint_idxs[method].append(e.step)\n",
    "#             return_means[method].append(e.value)\n",
    "\n",
    "#         for e in event_acc.Scalars(f\"evaluation/seed_{test_seed}/episodic_return_std\"):\n",
    "#             # checkpoint_idxs[method].append(e.step)\n",
    "#             return_stds[method].append(e.value)\n",
    "#     except:\n",
    "#         return_means[method].append(1)\n",
    "#         return_stds[method].append(1)\n",
    "#         continue\n",
    "\n",
    "\n",
    "env_config_id = \"train\"\n",
    "\n",
    "# checkpoint_idxs = defaultdict(list)\n",
    "\n",
    "for method in [\"stacked\", \"stacked_bdr\"]:\n",
    "    for test_env_config_id in test_env_config_ids:\n",
    "        checkpoint_directory = f\"../../runs/test/seed_{training_seed}/{env_id}/{test_env_config_id}/{env_config_id}/{method}/checkpoint_50000\"\n",
    "        if not os.path.exists(checkpoint_directory):\n",
    "            continue\n",
    "        filenames = os.listdir(checkpoint_directory)\n",
    "        for filename in filenames:\n",
    "            if filename.startswith(\"event\"):\n",
    "                event_filename = filename\n",
    "                break\n",
    "        event_filename = os.path.join(checkpoint_directory, event_filename)\n",
    "\n",
    "        event_acc = EventAccumulator(event_filename)\n",
    "        event_acc.Reload()\n",
    "\n",
    "        for e in event_acc.Scalars(f\"charts/checkpoint_50000/episodic_return\"):\n",
    "            # checkpoint_idxs[method].append(e.step)\n",
    "            returns[method].append(e.value)\n",
    "\n",
    "        for e in event_acc.Scalars(f\"evaluation/seed_{test_seed}/episodic_return_mean\"):\n",
    "            # checkpoint_idxs[method].append(e.step)\n",
    "            return_means[method].append(e.value)\n",
    "    \n",
    "for method in returns:\n",
    "    if method == 'optimal':\n",
    "        continue\n",
    "    # Create the heatmap\n",
    "    print(method)\n",
    "    k = 0.1\n",
    "    worst_k = int(len(returns[method])*k)\n",
    "    worst_k_returns = sorted(returns[method])[:worst_k]\n",
    "\n",
    "    print(f'Mean Episodic Reward: {np.mean(return_means[method])} +- {np.std(return_means[method])}')\n",
    "    print(f'Minimum Episodic Reward: {np.min(return_means[method])}')\n",
    "    print(f'Mean Episodic Reward (Worst {int(k*100)}%): {np.mean(worst_k_returns)} +- {np.std(worst_k_returns)}')\n",
    "    print('='*40)\n",
    "\n",
    "    # optimality_gap = np.array(return_means['optimal']) - np.array(return_means[method])\n",
    "    # optimal_worst_k = int(len(returns['optimal'])*k)\n",
    "    # optimal_worst_k_returns = sorted(returns['optimal'])[:optimal_worst_k]\n",
    "    # worst_k_optimal_gap = np.array(optimal_worst_k_returns) - np.array(worst_k_returns)\n",
    "    \n",
    "    # print(f'Mean Optimality Gap: {np.mean(optimality_gap)} +- {np.std(optimality_gap)}')\n",
    "    # print(f'Maximum Optimality Gap: {np.max(optimality_gap)}')\n",
    "    # print(f'Mean Optimality Gap (Worst {int(k*100)}%): {np.mean(worst_k_optimal_gap)}')\n",
    "    # print('='*40)\n",
    "    # print('='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seed = 1\n",
    "test_seed = 101\n",
    "env_id = \"CARLDmcWalkerEnv\"\n",
    "test_env_config_ids = [f\"test_{i}\" for i in range(1, 25+1)]\n",
    "c1_values = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "c2_values = [1, 5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config_id = \"default\"\n",
    "\n",
    "# checkpoint_idxs = defaultdict(list)\n",
    "return_means = defaultdict(list)\n",
    "return_stds = defaultdict(list)\n",
    "returns = defaultdict(list)\n",
    "\n",
    "\n",
    "# method = 'optimal'\n",
    "# for test_env_config_id in test_env_config_ids:\n",
    "#     checkpoint_directory = f\"../../runs/test/seed_{training_seed}/{env_id}/{test_env_config_id}/{test_env_config_id}/stacked\"\n",
    "#     if not os.path.exists(checkpoint_directory):\n",
    "#         return_means[method].append(1)\n",
    "#         return_stds[method].append(1)\n",
    "#         continue\n",
    "#     filenames = os.listdir(checkpoint_directory)\n",
    "#     for filename in filenames:\n",
    "#         if filename.startswith(\"event\"):\n",
    "#             event_filename = filename\n",
    "#             break\n",
    "#     event_filename = os.path.join(checkpoint_directory, event_filename)\n",
    "\n",
    "#     event_acc = EventAccumulator(event_filename)\n",
    "#     event_acc.Reload()\n",
    "\n",
    "#     # print(event_acc.Tags())\n",
    "\n",
    "#     try:\n",
    "#         for e in event_acc.Scalars(f\"charts/episodic_return\"):\n",
    "#             # checkpoint_idxs[method].append(e.step)\n",
    "#             returns[method].append(e.value)\n",
    "            \n",
    "#         for e in event_acc.Scalars(f\"evaluation/seed_{test_seed}/episodic_return_mean\"):\n",
    "#             # checkpoint_idxs[method].append(e.step)\n",
    "#             return_means[method].append(e.value)\n",
    "\n",
    "#         for e in event_acc.Scalars(f\"evaluation/seed_{test_seed}/episodic_return_std\"):\n",
    "#             # checkpoint_idxs[method].append(e.step)\n",
    "#             return_stds[method].append(e.value)\n",
    "#     except:\n",
    "#         return_means[method].append(1)\n",
    "#         return_stds[method].append(1)\n",
    "#         continue\n",
    "\n",
    "\n",
    "env_config_id = \"train\"\n",
    "\n",
    "# checkpoint_idxs = defaultdict(list)\n",
    "\n",
    "for method in [\"stacked\", \"stacked_bdr\"]:\n",
    "    for test_env_config_id in test_env_config_ids:\n",
    "        checkpoint_directory = f\"../../runs/test/seed_{training_seed}/{env_id}/{test_env_config_id}/{env_config_id}/{method}/checkpoint_500000\"\n",
    "        if not os.path.exists(checkpoint_directory):\n",
    "            continue\n",
    "        filenames = os.listdir(checkpoint_directory)\n",
    "        for filename in filenames:\n",
    "            if filename.startswith(\"event\"):\n",
    "                event_filename = filename\n",
    "                break\n",
    "        event_filename = os.path.join(checkpoint_directory, event_filename)\n",
    "\n",
    "        event_acc = EventAccumulator(event_filename)\n",
    "        event_acc.Reload()\n",
    "\n",
    "        for e in event_acc.Scalars(f\"charts/checkpoint_500000/episodic_return\"):\n",
    "            # checkpoint_idxs[method].append(e.step)\n",
    "            returns[method].append(e.value)\n",
    "\n",
    "        for e in event_acc.Scalars(f\"evaluation/seed_{test_seed}/episodic_return_mean\"):\n",
    "            # checkpoint_idxs[method].append(e.step)\n",
    "            return_means[method].append(e.value)\n",
    "    \n",
    "for method in returns:\n",
    "    if method == 'optimal':\n",
    "        continue\n",
    "    # Create the heatmap\n",
    "    print(method)\n",
    "    k = 0.1\n",
    "    worst_k = int(len(returns[method])*k)\n",
    "    worst_k_returns = sorted(returns[method])[:worst_k]\n",
    "\n",
    "    print(f'Mean Episodic Reward: {np.mean(return_means[method])} +- {np.std(return_means[method])}')\n",
    "    print(f'Minimum Episodic Reward: {np.min(return_means[method])}')\n",
    "    print(f'Mean Episodic Reward (Worst {int(k*100)}%): {np.mean(worst_k_returns)} +- {np.std(worst_k_returns)}')\n",
    "    print('='*40)\n",
    "\n",
    "    # optimality_gap = np.array(return_means['optimal']) - np.array(return_means[method])\n",
    "    # optimal_worst_k = int(len(returns['optimal'])*k)\n",
    "    # optimal_worst_k_returns = sorted(returns['optimal'])[:optimal_worst_k]\n",
    "    # worst_k_optimal_gap = np.array(optimal_worst_k_returns) - np.array(worst_k_returns)\n",
    "    \n",
    "    # print(f'Mean Optimality Gap: {np.mean(optimality_gap)} +- {np.std(optimality_gap)}')\n",
    "    # print(f'Maximum Optimality Gap: {np.max(optimality_gap)}')\n",
    "    # print(f'Mean Optimality Gap (Worst {int(k*100)}%): {np.mean(worst_k_optimal_gap)}')\n",
    "    # print('='*40)\n",
    "    # print('='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
